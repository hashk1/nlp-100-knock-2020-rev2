{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJyU1R_EqnCM"
   },
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYsnByE4ts0n"
   },
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgP3x5Bnt2Xg"
   },
   "outputs": [],
   "source": [
    "# ライブラリ読み込み\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28943,
     "status": "ok",
     "timestamp": 1609988730591,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "e-bRK7vxtr3i",
    "outputId": "6b33bbc1-56ce-4af8-8760-9b0f9714ace7"
   },
   "outputs": [],
   "source": [
    "# データ取得\n",
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "! wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "! unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgQZEBk3uF5t"
   },
   "outputs": [],
   "source": [
    "df = pd.read_table(\"newsCorpora.csv\", header=None)\n",
    "df.columns = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "df = df.query('PUBLISHER in [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]')\n",
    "df = df[[\"CATEGORY\", \"TITLE\"]]\n",
    "df[\"CATEGORY\"] = df[\"CATEGORY\"].map({\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3})\n",
    "\n",
    "X = df\n",
    "y = df[\"CATEGORY\"]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size =0.8, stratify=y, random_state=0)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, train_size =0.5, stratify=y_valid, random_state=0)\n",
    "\n",
    "X_train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n",
    "X_valid.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n",
    "X_test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIET-mJ9txpZ"
   },
   "source": [
    "## ここから本番"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmOk1WY1qirz"
   },
   "outputs": [],
   "source": [
    "# ライブラリ読み込み\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znWipQaMtZaP"
   },
   "source": [
    "### 70. 単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVw-0JyAqnsh"
   },
   "outputs": [],
   "source": [
    "# データ\n",
    "X_train = pd.read_table(\"train.txt\", header=None)\n",
    "X_valid = pd.read_table(\"valid.txt\", header=None)\n",
    "X_test = pd.read_table(\"test.txt\", header=None)\n",
    "X_train.columns = [\"CATEGORY\", \"TITLE\"]\n",
    "X_valid.columns = [\"CATEGORY\", \"TITLE\"]\n",
    "X_test.columns = [\"CATEGORY\", \"TITLE\"]\n",
    "\n",
    "# モデル\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg1Qu0Qduc0F"
   },
   "outputs": [],
   "source": [
    "def calc_docvec_from_row(row):\n",
    "    global model\n",
    "    wvs = [model[w] for w in row[\"TITLE\"].split() if w in model.vocab]\n",
    "    return np.nanmean(wvs, axis=0) if len(wvs) > 0 else np.zeros(shape=(model.vector_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1985,
     "status": "ok",
     "timestamp": 1609988890618,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "VTRu9AfLthWJ",
    "outputId": "3e0f9a61-43a1-4707-9a96-a7c3ecabdf7a"
   },
   "outputs": [],
   "source": [
    "docvec_train = X_train.progress_apply(calc_docvec_from_row, axis=1)\n",
    "docvec_valid = X_valid.progress_apply(calc_docvec_from_row, axis=1)\n",
    "docvec_test = X_test.progress_apply(calc_docvec_from_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1609988893403,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "SrAbOqS3ukPM",
    "outputId": "9c327c71-fcb0-4c43-e17f-b1abd47288c3"
   },
   "outputs": [],
   "source": [
    "# データ保存\n",
    "joblib.dump(np.array(docvec_train.tolist()), \"X_train.joblib\")\n",
    "joblib.dump(np.array(docvec_valid.tolist()), \"X_valid.joblib\")\n",
    "joblib.dump(np.array(docvec_test.tolist()), \"X_test.joblib\")\n",
    "joblib.dump(np.array(X_train[\"CATEGORY\"]), \"y_train.joblib\")\n",
    "joblib.dump(np.array(X_valid[\"CATEGORY\"]), \"y_valid.joblib\")\n",
    "joblib.dump(np.array(X_test[\"CATEGORY\"]), \"y_test.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeAAciyEuuq-"
   },
   "source": [
    "### 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54agk0UbunYk"
   },
   "outputs": [],
   "source": [
    "class SLPNet(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "    nn.init.normal_(self.fc.weight, 0.0, 1.0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1378,
     "status": "ok",
     "timestamp": 1609988903846,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "p49WI1Beu15p",
    "outputId": "7ec23f31-ae7f-4fd2-a525-4b62ef9175ae"
   },
   "outputs": [],
   "source": [
    "X_train = joblib.load(\"X_train.joblib\")\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n",
    "\n",
    "model = SLPNet(X_train.size()[1], 4)\n",
    "\n",
    "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
    "print(\"y-hat-1: \\n{}\".format(y_hat_1))\n",
    "Y_hat = torch.softmax(model(X_train[:4]), dim=-1)\n",
    "print(\"Y-hat: \\n{}\".format(Y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4We40UNu607"
   },
   "source": [
    "### 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1609989815817,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "0aDLivNiu4Yn",
    "outputId": "7ca6d3c8-65c6-4585-9b87-bcf3ab79594c"
   },
   "outputs": [],
   "source": [
    "X_train = joblib.load(\"X_train.joblib\")\n",
    "y_train = joblib.load(\"y_train.joblib\")\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n",
    "\n",
    "model = SLPNet(X_train.size()[1], 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_1 = criterion(model(X_train[:1]), y_train[:1])\n",
    "model.zero_grad()\n",
    "loss_1.backward()\n",
    "print(\"cross entropy loss from x_1: {}\".format(loss_1))\n",
    "print(\"gradient from x_1: {}\".format(model.fc.weight.grad))\n",
    "\n",
    "loss_4 = criterion(model(X_train[:4]), y_train[:4])\n",
    "model.zero_grad()\n",
    "loss_4.backward(retain_graph=True)\n",
    "print(\"cross entropy loss from X_[1:4]: {}\".format(loss_4))\n",
    "print(\"gradient from X_[1:4]: {}\".format(model.fc.weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A63rTzhXvCSZ"
   },
   "source": [
    "### 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xsx4jH4u-6v"
   },
   "outputs": [],
   "source": [
    "# 学習データ\n",
    "X_train = joblib.load(\"X_train.joblib\")\n",
    "y_train = joblib.load(\"y_train.joblib\")\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n",
    "y_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274442,
     "status": "ok",
     "timestamp": 1609991859348,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "yQXWaQ0WvEjZ",
    "outputId": "ef31a6fe-b985-48ce-b5f2-1def5cf998ea"
   },
   "outputs": [],
   "source": [
    "model = SLPNet(X_train.size()[1], 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "  \n",
    "  model.train()   \n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model(X), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    print(\"epoch: {}\\ttrain_loss: {}\".format(epoch+1, float(criterion(model(X_train), y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4PUcy9evJNb"
   },
   "source": [
    "### 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4WBwmfevK3x"
   },
   "outputs": [],
   "source": [
    "# 検証データ\n",
    "X_test = joblib.load(\"X_test.joblib\")\n",
    "y_test = joblib.load(\"y_test.joblib\")\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).clone()\n",
    "y_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1609992095532,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "lqJb-nJYvGwa",
    "outputId": "4bea10be-2278-41b1-ffbb-5dd561b5ff60"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(\"accuracy for train: {}\".format((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "    print(\"accuracy for test: {}\".format((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48rn1vuLvSGq"
   },
   "source": [
    "### 75. 損失と正解率のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273967,
     "status": "ok",
     "timestamp": 1609992388452,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "pw6HmGrevUBV",
    "outputId": "d8c42cd5-3330-465a-e468-644da9196690"
   },
   "outputs": [],
   "source": [
    "model = SLPNet(X_train.size()[1], 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epoch = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(n_epoch):\n",
    "  \n",
    "  model.train()\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model(X), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_losses.append(float(criterion(model(X_train), y_train)))\n",
    "    test_losses.append(float(criterion(model(X_test), y_test)))\n",
    "    train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "    test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
    "    print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 1155,
     "status": "ok",
     "timestamp": 1609992494327,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "FLU6DrJszSqI",
    "outputId": "6900ee69-21d0-495f-f630-bfda196ffe1d"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(test_losses, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accuracies, label=\"train accuracy\")\n",
    "plt.plot(test_accuracies, label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYCiARY40bzR"
   },
   "source": [
    "### 76. チェックポイント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64442,
     "status": "ok",
     "timestamp": 1609992789149,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "CQSqu1dx0bFB",
    "outputId": "109fdc59-cd14-49bd-8157-907bcaa6379d"
   },
   "outputs": [],
   "source": [
    "model = SLPNet(X_train.size()[1], 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epoch = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(n_epoch):\n",
    "\n",
    "  model.train()\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model(X), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    torch.save({\"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
    "    train_losses.append(float(criterion(model(X_train), y_train)))\n",
    "    test_losses.append(float(criterion(model(X_test), y_test)))\n",
    "    train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "    test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
    "    print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lljgn_FBDLC1"
   },
   "source": [
    "### 77. ミニバッチ化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUg1-MjGINLM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595709,
     "status": "ok",
     "timestamp": 1609993600285,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "H67ftI2HCl5P",
    "outputId": "96d8ab44-8804-4694-99b6-8c6f8e18434a"
   },
   "outputs": [],
   "source": [
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  model = SLPNet(X_train.size()[1], 4)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "  n_epoch = 100\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  elapsed_time = 0\n",
    "  for epoch in range(n_epoch):\n",
    "\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for X, y in dataloader_train:\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(model(X), y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    e_time = time.time()\n",
    "    elapsed_time += e_time - s_time\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      #torch.save({\"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
    "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
    "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
    "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
    "      #print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
    "\n",
    "  elapsed_time /= n_epoch\n",
    "  print(\"batch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(epoch+1, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VWTYZH3IPZo"
   },
   "source": [
    "### 78. GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614254,
     "status": "ok",
     "timestamp": 1609996203671,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "pJD5PuNUIOUG",
    "outputId": "ee765bdf-1231-4f65-a421-e438ac6084c8"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "  model = SLPNet(X_train.size()[1], 4).to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "  n_epoch = 100\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(n_epoch):\n",
    "\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for X, y in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    e_time = time.time()\n",
    "    elapsed_time = e_time - s_time\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      #torch.save({\"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
    "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
    "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
    "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
    "      #print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
    "\n",
    "  elapsed_time /= n_epoch\n",
    "  print(\"batch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(epoch+1, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVrex556Ktm_"
   },
   "source": [
    "### 79. 多層ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1609997230622,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "JGku3qi0LthL"
   },
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(input_size, 100, bias=True),\n",
    "        nn.PReLU(),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.Linear(100, 25, bias=True),\n",
    "        nn.PReLU(),\n",
    "        nn.BatchNorm1d(25),\n",
    "        nn.Linear(25, output_size, bias=True)\n",
    "    )\n",
    "\n",
    "    def init_normal(m):\n",
    "      if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, 0.0, 1.0)\n",
    "    self.fc.apply(init_normal)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83004,
     "status": "ok",
     "timestamp": 1609997342794,
     "user": {
      "displayName": "Koichi Hashimoto",
      "photoUrl": "",
      "userId": "17727187369180495955"
     },
     "user_tz": -540
    },
    "id": "k0ib0DtaKw19",
    "outputId": "47a069fc-b82a-44b5-e5be-aba236894b37"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "\n",
    "batch_sizes = [64, 128, 256]\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  \n",
    "  model = MLPNet(X_train.size()[1], 4).to(device)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "  n_epoch = 100\n",
    "  train_losses = []\n",
    "  test_losses = []\n",
    "  train_accuracies = []\n",
    "  test_accuracies = []\n",
    "  for epoch in range(n_epoch):\n",
    "\n",
    "    model.train()\n",
    "    s_time = time.time()\n",
    "    for X, y in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    e_time = time.time()\n",
    "    elapsed_time = e_time - s_time\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      #torch.save({\"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
    "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
    "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
    "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
    "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
    "      #print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
    "\n",
    "  elapsed_time /= n_epoch\n",
    "  print(\"batch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(epoch+1, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qO7X5_50pKD-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM43egpZytYDQWnta53O17s",
   "collapsed_sections": [],
   "name": "08-ニューラルネット.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
