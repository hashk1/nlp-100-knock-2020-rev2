{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "08-ニューラルネット.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGFz4zdWyJnnJqREkMyTkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hashk1/nlp-100-knock-2020-rev2/blob/main/08-%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJyU1R_EqnCM"
      },
      "source": [
        "# 第8章: ニューラルネット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYsnByE4ts0n"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgP3x5Bnt2Xg"
      },
      "source": [
        "# ライブラリ読み込み\n",
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-bRK7vxtr3i"
      },
      "source": [
        "# データ取得\n",
        "! wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "! wget -c https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "! unzip -o -d NewsAggregatorDataset NewsAggregatorDataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgQZEBk3uF5t"
      },
      "source": [
        "df = pd.read_table(\"NewsAggregatorDataset/newsCorpora.csv\", header=None)\n",
        "df.columns = [\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
        "df = df.query('PUBLISHER in [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]')\n",
        "df = df[[\"CATEGORY\", \"TITLE\"]]\n",
        "df[\"CATEGORY\"] = df[\"CATEGORY\"].map({\"b\": 0, \"t\": 1, \"e\": 2, \"m\": 3})\n",
        "\n",
        "X = df\n",
        "y = df[\"CATEGORY\"]\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size =0.8, stratify=y, random_state=0)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, train_size =0.5, stratify=y_valid, random_state=0)\n",
        "\n",
        "X_train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n",
        "X_valid.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n",
        "X_test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIET-mJ9txpZ"
      },
      "source": [
        "## ここから本番"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmOk1WY1qirz"
      },
      "source": [
        "# ライブラリ読み込み\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from gensim.models import KeyedVectors\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znWipQaMtZaP"
      },
      "source": [
        "### 70. 単語ベクトルの和による特徴量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVw-0JyAqnsh"
      },
      "source": [
        "# データ\n",
        "X_train = pd.read_table(\"train.txt\", header=None)\n",
        "X_valid = pd.read_table(\"valid.txt\", header=None)\n",
        "X_test = pd.read_table(\"test.txt\", header=None)\n",
        "X_train.columns = [\"CATEGORY\", \"TITLE\"]\n",
        "X_valid.columns = [\"CATEGORY\", \"TITLE\"]\n",
        "X_test.columns = [\"CATEGORY\", \"TITLE\"]\n",
        "\n",
        "# モデル\n",
        "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg1Qu0Qduc0F"
      },
      "source": [
        "def calc_docvec_from_row(row):\n",
        "    global model\n",
        "    wvs = [model[w] for w in row[\"TITLE\"].split() if w in model.vocab]\n",
        "    return np.nanmean(wvs, axis=0) if len(wvs) > 0 else np.zeros(shape=(model.vector_size, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTRu9AfLthWJ"
      },
      "source": [
        "docvec_train = X_train.progress_apply(calc_docvec_from_row, axis=1)\n",
        "docvec_valid = X_valid.progress_apply(calc_docvec_from_row, axis=1)\n",
        "docvec_test = X_test.progress_apply(calc_docvec_from_row, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrAbOqS3ukPM"
      },
      "source": [
        "# データ保存\n",
        "joblib.dump(np.array(docvec_train.tolist()), \"X_train.joblib\")\n",
        "joblib.dump(np.array(docvec_valid.tolist()), \"X_valid.joblib\")\n",
        "joblib.dump(np.array(docvec_test.tolist()), \"X_test.joblib\")\n",
        "joblib.dump(np.array(X_train[\"CATEGORY\"]), \"y_train.joblib\")\n",
        "joblib.dump(np.array(X_valid[\"CATEGORY\"]), \"y_valid.joblib\")\n",
        "joblib.dump(np.array(X_test[\"CATEGORY\"]), \"y_test.joblib\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeAAciyEuuq-"
      },
      "source": [
        "### 71. 単層ニューラルネットワークによる予測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvc5mDOMyTha"
      },
      "source": [
        "# 以下で使う\n",
        "\n",
        "# 学習データ\n",
        "X_train = joblib.load(\"X_train.joblib\")\n",
        "y_train = joblib.load(\"y_train.joblib\")\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n",
        "y_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n",
        "dataset_train = TensorDataset(X_train, y_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
        "\n",
        "# 検証データ\n",
        "X_test = joblib.load(\"X_test.joblib\")\n",
        "y_test = joblib.load(\"y_test.joblib\")\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32)).clone()\n",
        "y_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n",
        "dataset_test = TensorDataset(X_test, y_test)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54agk0UbunYk"
      },
      "source": [
        "class SLPNet(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Linear(input_size, output_size, bias=False)\n",
        "    nn.init.normal_(self.fc.weight, 0.0, 1.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p49WI1Beu15p"
      },
      "source": [
        "model = SLPNet(X_train.size()[1], 4)\n",
        "\n",
        "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
        "print(\"y-hat-1: \\n{}\".format(y_hat_1))\n",
        "Y_hat = torch.softmax(model(X_train[:4]), dim=-1)\n",
        "print(\"Y-hat: \\n{}\".format(Y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4We40UNu607"
      },
      "source": [
        "### 72. 損失と勾配の計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aDLivNiu4Yn"
      },
      "source": [
        "model = SLPNet(X_train.size()[1], 4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_1 = criterion(model(X_train[:1]), y_train[:1])\n",
        "model.zero_grad()\n",
        "loss_1.backward()\n",
        "print(\"cross entropy loss from x_1: {}\".format(loss_1))\n",
        "print(\"gradient from x_1: {}\".format(model.fc.weight.grad))\n",
        "\n",
        "loss_4 = criterion(model(X_train[:4]), y_train[:4])\n",
        "model.zero_grad()\n",
        "loss_4.backward(retain_graph=True)\n",
        "print(\"cross entropy loss from X_[1:4]: {}\".format(loss_4))\n",
        "print(\"gradient from X_[1:4]: {}\".format(model.fc.weight.grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A63rTzhXvCSZ"
      },
      "source": [
        "### 73. 確率的勾配降下法による学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQXWaQ0WvEjZ"
      },
      "source": [
        "model = SLPNet(X_train.size()[1], 4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epoch = 100\n",
        "for epoch in range(n_epoch):\n",
        "  \n",
        "  model.train()   \n",
        "  for X, y in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(X), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    print(\"epoch: {}\\ttrain_loss: {}\".format(epoch+1, float(criterion(model(X_train), y_train))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4PUcy9evJNb"
      },
      "source": [
        "### 74. 正解率の計測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqJb-nJYvGwa"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(\"accuracy for train: {}\".format((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "    print(\"accuracy for test: {}\".format((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rn1vuLvSGq"
      },
      "source": [
        "### 75. 損失と正解率のプロット"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw6HmGrevUBV"
      },
      "source": [
        "model = SLPNet(X_train.size()[1], 4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epoch = 100\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "for epoch in range(n_epoch):\n",
        "  \n",
        "  model.train()\n",
        "  for X, y in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(X), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_losses.append(float(criterion(model(X_train), y_train)))\n",
        "    test_losses.append(float(criterion(model(X_test), y_test)))\n",
        "    train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "    test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
        "    print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLU6DrJszSqI"
      },
      "source": [
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(test_losses, label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(train_accuracies, label=\"train accuracy\")\n",
        "plt.plot(test_accuracies, label=\"test accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYCiARY40bzR"
      },
      "source": [
        "### 76. チェックポイント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQSqu1dx0bFB"
      },
      "source": [
        "model = SLPNet(X_train.size()[1], 4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "n_epoch = 100\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "  model.train()\n",
        "  for X, y in dataloader_train:\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(model(X), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    torch.save({\"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
        "    train_losses.append(float(criterion(model(X_train), y_train)))\n",
        "    test_losses.append(float(criterion(model(X_test), y_test)))\n",
        "    train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "    test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
        "    print(\"epoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lljgn_FBDLC1"
      },
      "source": [
        "### 77. ミニバッチ化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUg1-MjGINLM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H67ftI2HCl5P"
      },
      "source": [
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "for batch_size in tqdm(batch_sizes):\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  model = SLPNet(X_train.size()[1], 4)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  n_epoch = 100\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  elapsed_time = 0\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    model.train()\n",
        "    s_time = time.time()\n",
        "    for X, y in dataloader_train:\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(model(X), y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    e_time = time.time()\n",
        "    elapsed_time += e_time - s_time\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      #torch.save({\"batch_size\": batch_size, \"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
        "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
        "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
        "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
        "      #print(\"batch_size: {}\\tepoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(batch_size, epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
        "\n",
        "  elapsed_time /= n_epoch\n",
        "  print(\"\\nbatch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(batch_size, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VWTYZH3IPZo"
      },
      "source": [
        "### 78. GPU上での学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJD5PuNUIOUG"
      },
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "dataset_train = TensorDataset(X_train, y_train)\n",
        "\n",
        "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "for batch_size in tqdm(batch_sizes):\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  \n",
        "  model = SLPNet(X_train.size()[1], 4).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  n_epoch = 100\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  elapsed_time = 0\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    model.train()\n",
        "    s_time = time.time()\n",
        "    for X, y in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(X), y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    e_time = time.time()\n",
        "    elapsed_time += e_time - s_time\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      #torch.save({\"batch_size\": batch_size, \"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
        "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
        "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
        "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
        "      #print(\"batch_size: {}\\tepoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(batch_size, epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
        "\n",
        "  elapsed_time /= n_epoch\n",
        "  print(\"\\nbatch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(batch_size, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVrex556Ktm_"
      },
      "source": [
        "### 79. 多層ニューラルネットワーク"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGku3qi0LthL"
      },
      "source": [
        "class MLPNet(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(input_size, 100, bias=True),\n",
        "        nn.PReLU(),\n",
        "        nn.BatchNorm1d(100),\n",
        "        nn.Linear(100, 25, bias=True),\n",
        "        nn.PReLU(),\n",
        "        nn.BatchNorm1d(25),\n",
        "        nn.Linear(25, output_size, bias=True)\n",
        "    )\n",
        "\n",
        "    def init_normal(m):\n",
        "      if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, 0.0, 1.0)\n",
        "    self.fc.apply(init_normal)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0ib0DtaKw19"
      },
      "source": [
        "device = \"cuda:0\"\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "X_test = X_test.to(device)\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "dataset_train = TensorDataset(X_train, y_train)\n",
        "\n",
        "batch_sizes = [64, 128, 256]\n",
        "for batch_size in tqdm(batch_sizes):\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  \n",
        "  model = MLPNet(X_train.size()[1], 4).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "  n_epoch = 100\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  elapsed_time = 0\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    model.train()\n",
        "    s_time = time.time()\n",
        "    for X, y in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    e_time = time.time()\n",
        "    elapsed_time += e_time - s_time\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      #torch.save({\"batch_size\": batch_size, \"epoch\": epoch+1, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict()}, \"checkpoint-{}.pytorch\".format(epoch+1)) \n",
        "      train_losses.append(float(criterion(model(X_train), y_train)))\n",
        "      test_losses.append(float(criterion(model(X_test), y_test)))\n",
        "      train_accuracies.append(float((model(X_train).max(axis=1).indices == y_train).sum() / len(y_train)))\n",
        "      test_accuracies.append(float((model(X_test).max(axis=1).indices == y_test).sum() / len(y_test)))\n",
        "      #print(\"batch_size: {}\\tepoch: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuracy: {}\\ttest_accuracy: {}\".format(batch_size, epoch+1, train_losses[epoch], test_losses[epoch], train_accuracies[epoch], test_accuracies[epoch])) \n",
        "\n",
        "  elapsed_time /= n_epoch\n",
        "  print(\"\\nbatch_size: {}\\telapsed_time: {}\\ttrain_loss: {}\\ttest_loss: {}\\ttrain_accuray: {}\\ttest_accuracy: {}\".format(batch_size, elapsed_time, train_losses[n_epoch-1], test_losses[n_epoch-1], train_accuracies[n_epoch-1], test_accuracies[n_epoch-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO7X5_50pKD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}